{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dab3bde2-7994-45b9-9956-36f8649f96e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=L5kIXRdwGAI\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=79rSUeJr1sE\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=l1uE_pBqnvE\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=y1sF6ZeASU0\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=aRBVzMluVnU\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=B5Ch3jk2u0s\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=nZxr3vngv88\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=2axQXJK-6R0\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=AM2-3ufgTCs\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=4Xm2jWeuKMc\n",
      "[{'channle_name': 'MrBeast', 'channle_id': 'UCX6OQ3DkcsbYNE6H8uQQuVA', 'title': 'Katana Vs Bullet', 'description': '', 'view': '132223407', 'like': '9037947', 'publish_data': '2023-09-07T17:00:01Z', 'link': 'https://www.youtube.com/watch?v=L5kIXRdwGAI', 'video_id': 'L5kIXRdwGAI'}, {'channle_name': 'Dylan Anderson', 'channle_id': 'UCpuT8AlP9P9EgW_pZ0_xInQ', 'title': 'This guy had his coworkers fooled üòÇ', 'description': '', 'view': '16241946', 'like': '973739', 'publish_data': '2023-09-10T01:46:27Z', 'link': 'https://www.youtube.com/watch?v=79rSUeJr1sE', 'video_id': '79rSUeJr1sE'}, {'channle_name': 'Ryan HD', 'channle_id': 'UC68zG933W7D9V9H9TLXBPZQ', 'title': 'Students Excuses For Not Doing Their Homework.', 'description': '#school #student #relatable #teacher #studentlife #studentmemes', 'view': '29989833', 'like': '1901993', 'publish_data': '2023-09-07T14:50:47Z', 'link': 'https://www.youtube.com/watch?v=l1uE_pBqnvE', 'video_id': 'l1uE_pBqnvE'}, {'channle_name': 'Vivziepop', 'channle_id': 'UCzfyYtgvkx5mLy8nlLlayYg', 'title': 'HELLUVA BOSS - OOPS // S2: Episode 6', 'description': 'Fizz goes to buy milk, and things sure do happen.\\nWARNING: for sad clowns.\\nHELLUVA BOSS MERCH AVAILABLE ‚ñ∫ https://sharkrobot.com/vivziepop\\n\\nWANT MORE? WATCH OUR OTHER EPISODES!\\n\\nPilot Episode ‚ñ∫ https://www.youtube.com/watch?v=OlahNrlcgS4\\nS1 Episode One ‚ñ∫ https://www.youtube.com/watch?v=el_PChGfJN8\\nS1 Episode Two ‚ñ∫ https://www.youtube.com/watch?v=kpnwRg268FQ\\nS1 Episode Three ‚ñ∫  https://www.youtube.com/watch?v=RghsgkZKedg\\nS1 Episode Four ‚ñ∫ https://www.youtube.com/watch?v=1ZFseYPmkAk\\nS1 Episode Five ‚ñ∫ https://www.youtube.com/watch?v=h2ZmVAdezF8\\nS1 Episode Six ‚ñ∫ https://youtu.be/yXErLiSbxXQ\\nS1 Episode Seven ‚ñ∫ https://youtu.be/8zyGQquL8VM\\nS2 Episode One ‚ñ∫ https://youtu.be/_spuxXnul0U\\nS2 Episode Two ‚ñ∫ https://youtu.be/4J0xFUyz1nw\\nS2 Episode Three ‚ñ∫ https://youtu.be/j1BfO7VlIw4\\nS2 Episode Four ‚ñ∫ https://youtu.be/KJy7T24rhg0\\nS2 Episode Five ‚ñ∫ https://youtu.be/lYu1ysDULwA\\n\\nOr Support on Patreon!\\nhttp://www.patreon.com/VivienneMedrano\\n\\nThumbnail by https://twitter.com/GasuGuma\\n\\n#HelluvaBoss #HelluvaBossSeason2 #SEASONTWO', 'view': '8910082', 'like': '727862', 'publish_data': '2023-09-09T16:00:51Z', 'link': 'https://www.youtube.com/watch?v=y1sF6ZeASU0', 'video_id': 'y1sF6ZeASU0'}, {'channle_name': 'Sleazy Pete', 'channle_id': 'UCsoGdClWwWeAXieOTt7sB-g', 'title': 'Mattie Finally Tracks Down Chaney! (1/3) ‚Ä¢ (13/18)‚îÉTrue Grit (2010)‚îÉ4K‚îÉ#sleazypete', 'description': '', 'view': '4000500', 'like': '187693', 'publish_data': '2023-09-10T19:44:23Z', 'link': 'https://www.youtube.com/watch?v=aRBVzMluVnU', 'video_id': 'aRBVzMluVnU'}, {'channle_name': 'Daily Dose Of Internet', 'channle_id': 'UCdC0An4ZPNr_YiFiYoVbwaw', 'title': \"Sleepwalker Won't Stop Staring\", 'description': 'Source: Collab.Inc\\n\\n#shorts', 'view': '8162233', 'like': '465168', 'publish_data': '2023-09-10T19:38:16Z', 'link': 'https://www.youtube.com/watch?v=B5Ch3jk2u0s', 'video_id': 'B5Ch3jk2u0s'}, {'channle_name': 'Ray William Johnson', 'channle_id': 'UCGt7X90Au6BV8rf49BiM6Dg', 'title': 'I can‚Äôt handle this drama', 'description': 'TikTok star claims she was denied entry by Deep Hollow Horse Ranch because of her weight \\n\\n#remibader #horse #horsebackriding #ranch #raywilliamjohnson #shorts', 'view': '9448587', 'like': '962415', 'publish_data': '2023-09-08T00:16:40Z', 'link': 'https://www.youtube.com/watch?v=nZxr3vngv88', 'video_id': 'nZxr3vngv88'}, {'channle_name': 'Cop Humor', 'channle_id': 'UCW2R_zesOCCKqkNj4lJZ77A', 'title': 'Man Uses Donuts to Fish for Cops - Hilarious Reaction! üòÇ #shorts', 'description': \"When this man (therealernest425 TT) noticed a police car parked nearby, he decided to have a bit of fun by fishing for cops, using a bag of donuts! But he never expected one of the police officers to react the way he did! Don't miss this hilarious police reaction caught on camera!\\n\\nPlease like, share, and subscribe for more hilarious videos! Thank you üòä\", 'view': '9661815', 'like': '480600', 'publish_data': '2023-09-08T19:17:32Z', 'link': 'https://www.youtube.com/watch?v=2axQXJK-6R0', 'video_id': '2axQXJK-6R0'}, {'channle_name': 'Worldwide Watchlist Movies', 'channle_id': 'UCSMZPYKUC1WXl5KseRBI9yA', 'title': 'Nolan visits elders about late rent üòØ', 'description': '#therookie #shorts', 'view': '2080394', 'like': '213690', 'publish_data': '2023-09-13T15:20:41Z', 'link': 'https://www.youtube.com/watch?v=AM2-3ufgTCs', 'video_id': 'AM2-3ufgTCs'}, {'channle_name': 'KEEMOKAZI', 'channle_id': 'UCMOLiZpKXp-w5CNfqJObQUw', 'title': 'Saying ‚ÄúNO‚Äù to my spoiled sisters üòÇ', 'description': '', 'view': '8065511', 'like': '496577', 'publish_data': '2023-09-10T22:38:32Z', 'link': 'https://www.youtube.com/watch?v=4Xm2jWeuKMc', 'video_id': '4Xm2jWeuKMc'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from IPython.display import JSON\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n",
    "\n",
    "# Disable OAuthlib's HTTPS verification when running locally.\n",
    "# *DO NOT* leave this option enabled in production.\n",
    "os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "api_key = \"AIzaSyBbxGwSTdF9VFHcHyarpxOAyFZ-tdCp_6o\"\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=api_key)\n",
    "\n",
    "request = youtube.videos().list(\n",
    "    part=\"snippet,contentDetails,statistics\",\n",
    "    chart=\"mostPopular\",\n",
    "    regionCode=\"US\",\n",
    "    maxResults=10,  # Set the number of results per page\n",
    "    videoCategoryId=\"24\"  # Set the video category to \"Shorts\"\n",
    ")\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "data = []\n",
    "for item in response['items']:\n",
    "    video_id = item[\"id\"]\n",
    "    video_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "    channle_name = item['snippet']['channelTitle']\n",
    "    channle_id = item['snippet']['channelId']\n",
    "    title = item['snippet']['localized']['title']\n",
    "    description = item['snippet']['localized']['description']\n",
    "    publish_data = item['snippet']['publishedAt']\n",
    "    view = item['statistics']['viewCount']\n",
    "    like = item['statistics'].get('likeCount', 0)  # Handle missing 'likeCount'\n",
    "\n",
    "    import json\n",
    "    \n",
    "    # Define the filename where your JSON data is stored\n",
    "    json_filename = 'video_id.json'\n",
    "    \n",
    "    # Step 1: Open the JSON file and load its contents into a Python dictionary\n",
    "    with open(json_filename, 'r') as json_file:\n",
    "        j_data = json.load(json_file)\n",
    "    \n",
    "    # Step 2: Update the 'video_id' key with a list of video IDs\n",
    "    if 'video_id' not in j_data:\n",
    "        j_data['video_id'] = []  # Initialize an empty list if 'video_id' doesn't exist\n",
    "    \n",
    "    # Add video IDs to the 'video_id' list if they don't already exist\n",
    "    video_ids_to_add = [video_id]\n",
    "    \n",
    "    for video_id in video_ids_to_add:\n",
    "        if video_id not in j_data['video_id']:\n",
    "            j_data['video_id'].append(video_id)\n",
    "    \n",
    "    # Step 3: Save the updated dictionary back to the JSON file\n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json.dump(j_data, json_file)\n",
    "    \n",
    "    print(f\"Updated JSON data saved to {json_filename}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    all_data = {\n",
    "        'channle_name':channle_name,\n",
    "        'channle_id':channle_id,\n",
    "        'title': title,\n",
    "        'description': description,\n",
    "        'view': view,\n",
    "        'like': like,\n",
    "        'publish_data': publish_data,\n",
    "        'link':video_link,\n",
    "        'video_id':video_id\n",
    "    }\n",
    "    data.append(all_data)\n",
    "    print(video_link)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaba5df4-e775-4320-a30d-260ae1254c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "151a7cbf-1824-41bf-906a-35fe50e98c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'channle_name': 'MrBeast',\n",
       " 'channle_id': 'UCX6OQ3DkcsbYNE6H8uQQuVA',\n",
       " 'title': 'Katana Vs Bullet',\n",
       " 'description': '',\n",
       " 'view': '131906494',\n",
       " 'like': '9017118',\n",
       " 'publish_data': '2023-09-07T17:00:01Z',\n",
       " 'link': 'https://www.youtube.com/watch?v=L5kIXRdwGAI',\n",
       " 'video_id': 'L5kIXRdwGAI'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5d3cfc-0574-44f8-96bc-d2b2458b37f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=L5kIXRdwGAI\n",
      "[youtube] L5kIXRdwGAI: Downloading webpage\n",
      "[youtube] L5kIXRdwGAI: Downloading ios player API JSON\n",
      "[youtube] L5kIXRdwGAI: Downloading android player API JSON\n",
      "[youtube] L5kIXRdwGAI: Downloading m3u8 information\n",
      "[info] L5kIXRdwGAI: Downloading 1 format(s): 616+251-13\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 8\n",
      "[download] Destination: youtube\\MrBeast\\Katana Vs Bullet.mp4.f616.mp4\n",
      "[download] 100% of   22.55MiB in 00:00:01 at 16.53MiB/s                 \n",
      "[download] Destination: youtube\\MrBeast\\Katana Vs Bullet.mp4.f251-13.webm\n",
      "[download] 100% of  627.35KiB in 00:00:00 at 4.64MiB/s   \n",
      "[Merger] Merging formats into \"youtube\\MrBeast\\Katana Vs Bullet.mp4.webm\"\n",
      "Deleting original file youtube\\MrBeast\\Katana Vs Bullet.mp4.f251-13.webm (pass -k to keep)\n",
      "Deleting original file youtube\\MrBeast\\Katana Vs Bullet.mp4.f616.mp4 (pass -k to keep)\n",
      "Metadata saved to youtube\\MrBeast\\metadata\\Katana Vs Bullet.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=79rSUeJr1sE\n",
      "[youtube] 79rSUeJr1sE: Downloading webpage\n",
      "[youtube] 79rSUeJr1sE: Downloading ios player API JSON\n",
      "[youtube] 79rSUeJr1sE: Downloading android player API JSON\n",
      "[youtube] 79rSUeJr1sE: Downloading m3u8 information\n",
      "[info] 79rSUeJr1sE: Downloading 1 format(s): 616+251\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 4\n",
      "[download] Destination: youtube\\Dylan Anderson\\This guy had his coworkers fooled .mp4.f616.mp4\n",
      "[download] 100% of    7.48MiB in 00:00:01 at 7.03MiB/s                  \n",
      "[download] Destination: youtube\\Dylan Anderson\\This guy had his coworkers fooled .mp4.f251.webm\n",
      "[download] 100% of  343.02KiB in 00:00:00 at 3.29MiB/s   \n",
      "[Merger] Merging formats into \"youtube\\Dylan Anderson\\This guy had his coworkers fooled .mp4.webm\"\n",
      "Deleting original file youtube\\Dylan Anderson\\This guy had his coworkers fooled .mp4.f251.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Dylan Anderson\\This guy had his coworkers fooled .mp4.f616.mp4 (pass -k to keep)\n",
      "Metadata saved to youtube\\Dylan Anderson\\metadata\\This guy had his coworkers fooled .csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=l1uE_pBqnvE\n",
      "[youtube] l1uE_pBqnvE: Downloading webpage\n",
      "[youtube] l1uE_pBqnvE: Downloading ios player API JSON\n",
      "[youtube] l1uE_pBqnvE: Downloading android player API JSON\n",
      "[youtube] l1uE_pBqnvE: Downloading m3u8 information\n",
      "[info] l1uE_pBqnvE: Downloading 1 format(s): 616+251\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 9\n",
      "[download] Destination: youtube\\Ryan HD\\Students Excuses For Not Doing Their Homework.mp4.f616.mp4\n",
      "[download] 100% of   11.39MiB in 00:00:01 at 8.23MiB/s                  \n",
      "[download] Destination: youtube\\Ryan HD\\Students Excuses For Not Doing Their Homework.mp4.f251.webm\n",
      "[download] 100% of  666.22KiB in 00:00:00 at 5.61MiB/s   \n",
      "[Merger] Merging formats into \"youtube\\Ryan HD\\Students Excuses For Not Doing Their Homework.mp4.webm\"\n",
      "Deleting original file youtube\\Ryan HD\\Students Excuses For Not Doing Their Homework.mp4.f251.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Ryan HD\\Students Excuses For Not Doing Their Homework.mp4.f616.mp4 (pass -k to keep)\n",
      "Metadata saved to youtube\\Ryan HD\\metadata\\Students Excuses For Not Doing Their Homework.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=y1sF6ZeASU0\n",
      "[youtube] y1sF6ZeASU0: Downloading webpage\n",
      "[youtube] y1sF6ZeASU0: Downloading ios player API JSON\n",
      "[youtube] y1sF6ZeASU0: Downloading android player API JSON\n",
      "[youtube] y1sF6ZeASU0: Downloading m3u8 information\n",
      "[info] y1sF6ZeASU0: Downloading 1 format(s): 271+251\n",
      "[download] Destination: youtube\\Vivziepop\\HELLUVA BOSS - OOPS  S2 Episode 6.mp4.f271.webm\n",
      "[download] 100% of  994.62MiB in 00:01:20 at 12.35MiB/s     \n",
      "[download] Destination: youtube\\Vivziepop\\HELLUVA BOSS - OOPS  S2 Episode 6.mp4.f251.webm\n",
      "[download] 100% of   25.14MiB in 00:00:00 at 25.58MiB/s    \n",
      "[Merger] Merging formats into \"youtube\\Vivziepop\\HELLUVA BOSS - OOPS  S2 Episode 6.mp4.webm\"\n",
      "Deleting original file youtube\\Vivziepop\\HELLUVA BOSS - OOPS  S2 Episode 6.mp4.f271.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Vivziepop\\HELLUVA BOSS - OOPS  S2 Episode 6.mp4.f251.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\Vivziepop\\metadata\\HELLUVA BOSS - OOPS  S2 Episode 6.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=B5Ch3jk2u0s\n",
      "[youtube] B5Ch3jk2u0s: Downloading webpage\n",
      "[youtube] B5Ch3jk2u0s: Downloading ios player API JSON\n",
      "[youtube] B5Ch3jk2u0s: Downloading android player API JSON\n",
      "[youtube] B5Ch3jk2u0s: Downloading m3u8 information\n",
      "[info] B5Ch3jk2u0s: Downloading 1 format(s): 616+251\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 4\n",
      "[download] Destination: youtube\\Daily Dose Of Internet\\Sleepwalker Wont Stop Staring.mp4.f616.mp4\n",
      "[download] 100% of    4.83MiB in 00:00:00 at 7.78MiB/s                  \n",
      "[download] Destination: youtube\\Daily Dose Of Internet\\Sleepwalker Wont Stop Staring.mp4.f251.webm\n",
      "[download] 100% of  261.87KiB in 00:00:00 at 2.85MiB/s   \n",
      "[Merger] Merging formats into \"youtube\\Daily Dose Of Internet\\Sleepwalker Wont Stop Staring.mp4.webm\"\n",
      "Deleting original file youtube\\Daily Dose Of Internet\\Sleepwalker Wont Stop Staring.mp4.f251.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Daily Dose Of Internet\\Sleepwalker Wont Stop Staring.mp4.f616.mp4 (pass -k to keep)\n",
      "Metadata saved to youtube\\Daily Dose Of Internet\\metadata\\Sleepwalker Wont Stop Staring.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=aRBVzMluVnU\n",
      "[youtube] aRBVzMluVnU: Downloading webpage\n",
      "[youtube] aRBVzMluVnU: Downloading ios player API JSON\n",
      "[youtube] aRBVzMluVnU: Downloading android player API JSON\n",
      "[youtube] aRBVzMluVnU: Downloading player afd1b6e5\n",
      "[youtube] aRBVzMluVnU: Downloading m3u8 information\n",
      "[info] aRBVzMluVnU: Downloading 1 format(s): 315+251\n",
      "[download] Destination: youtube\\Sleazy Pete\\Mattie Finally Tracks Down Chaney 13  1318True Grit 20104Ksleazypete.mp4.f315.webm\n",
      "[download] 100% of  122.71MiB in 00:01:40 at 1.22MiB/s   \n",
      "[download] Destination: youtube\\Sleazy Pete\\Mattie Finally Tracks Down Chaney 13  1318True Grit 20104Ksleazypete.mp4.f251.webm\n",
      "[download] 100% of  890.67KiB in 00:00:00 at 6.92MiB/s   \n",
      "[Merger] Merging formats into \"youtube\\Sleazy Pete\\Mattie Finally Tracks Down Chaney 13  1318True Grit 20104Ksleazypete.mp4.webm\"\n",
      "Deleting original file youtube\\Sleazy Pete\\Mattie Finally Tracks Down Chaney 13  1318True Grit 20104Ksleazypete.mp4.f315.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Sleazy Pete\\Mattie Finally Tracks Down Chaney 13  1318True Grit 20104Ksleazypete.mp4.f251.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\Sleazy Pete\\metadata\\Mattie Finally Tracks Down Chaney 13  1318True Grit 20104Ksleazypete.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=nZxr3vngv88\n",
      "[youtube] nZxr3vngv88: Downloading webpage\n",
      "[youtube] nZxr3vngv88: Downloading ios player API JSON\n",
      "[youtube] nZxr3vngv88: Downloading android player API JSON\n",
      "[youtube] nZxr3vngv88: Downloading m3u8 information\n",
      "[info] nZxr3vngv88: Downloading 1 format(s): 616+251\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 12\n",
      "[download] Destination: youtube\\Ray William Johnson\\I cant handle this drama.mp4.f616.mp4\n",
      "[download] 100% of   21.00MiB in 00:00:02 at 9.76MiB/s                    \n",
      "[download] Destination: youtube\\Ray William Johnson\\I cant handle this drama.mp4.f251.webm\n",
      "[download] 100% of  876.50KiB in 00:00:00 at 6.37MiB/s     \n",
      "[Merger] Merging formats into \"youtube\\Ray William Johnson\\I cant handle this drama.mp4.webm\"\n",
      "Deleting original file youtube\\Ray William Johnson\\I cant handle this drama.mp4.f616.mp4 (pass -k to keep)\n",
      "Deleting original file youtube\\Ray William Johnson\\I cant handle this drama.mp4.f251.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\Ray William Johnson\\metadata\\I cant handle this drama.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=2axQXJK-6R0\n",
      "[youtube] 2axQXJK-6R0: Downloading webpage\n",
      "[youtube] 2axQXJK-6R0: Downloading ios player API JSON\n",
      "[youtube] 2axQXJK-6R0: Downloading android player API JSON\n",
      "[youtube] 2axQXJK-6R0: Downloading m3u8 information\n",
      "[info] 2axQXJK-6R0: Downloading 1 format(s): 247+251\n",
      "[download] Destination: youtube\\Cop Humor\\Man Uses Donuts to Fish for Cops - Hilarious Reaction  shorts.mp4.f247.webm\n",
      "[download] 100% of    1.16MiB in 00:00:00 at 8.21MiB/s     \n",
      "[download] Destination: youtube\\Cop Humor\\Man Uses Donuts to Fish for Cops - Hilarious Reaction  shorts.mp4.f251.webm\n",
      "[download] 100% of  239.57KiB in 00:00:00 at 1.74MiB/s   \n",
      "[Merger] Merging formats into \"youtube\\Cop Humor\\Man Uses Donuts to Fish for Cops - Hilarious Reaction  shorts.mp4.webm\"\n",
      "Deleting original file youtube\\Cop Humor\\Man Uses Donuts to Fish for Cops - Hilarious Reaction  shorts.mp4.f251.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Cop Humor\\Man Uses Donuts to Fish for Cops - Hilarious Reaction  shorts.mp4.f247.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\Cop Humor\\metadata\\Man Uses Donuts to Fish for Cops - Hilarious Reaction  shorts.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=AM2-3ufgTCs\n",
      "[youtube] AM2-3ufgTCs: Downloading webpage\n",
      "[youtube] AM2-3ufgTCs: Downloading ios player API JSON\n",
      "[youtube] AM2-3ufgTCs: Downloading android player API JSON\n",
      "[youtube] AM2-3ufgTCs: Downloading m3u8 information\n",
      "[info] AM2-3ufgTCs: Downloading 1 format(s): 303+251\n",
      "[download] Destination: youtube\\Worldwide Watchlist Movies\\Nolan visits elders about late rent .mp4.f303.webm\n",
      "[download] 100% of    8.38MiB in 00:00:00 at 19.77MiB/s    \n",
      "[download] Destination: youtube\\Worldwide Watchlist Movies\\Nolan visits elders about late rent .mp4.f251.webm\n",
      "[download] 100% of  708.19KiB in 00:00:00 at 6.11MiB/s     \n",
      "[Merger] Merging formats into \"youtube\\Worldwide Watchlist Movies\\Nolan visits elders about late rent .mp4.webm\"\n",
      "Deleting original file youtube\\Worldwide Watchlist Movies\\Nolan visits elders about late rent .mp4.f303.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Worldwide Watchlist Movies\\Nolan visits elders about late rent .mp4.f251.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\Worldwide Watchlist Movies\\metadata\\Nolan visits elders about late rent .csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=4Xm2jWeuKMc\n",
      "[youtube] 4Xm2jWeuKMc: Downloading webpage\n",
      "[youtube] 4Xm2jWeuKMc: Downloading ios player API JSON\n",
      "[youtube] 4Xm2jWeuKMc: Downloading android player API JSON\n",
      "[youtube] 4Xm2jWeuKMc: Downloading m3u8 information\n",
      "[info] 4Xm2jWeuKMc: Downloading 1 format(s): 616+251\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 7\n",
      "[download] Destination: youtube\\KEEMOKAZI\\Saying NO to my spoiled sisters .mp4.f616.mp4\n",
      "[download] 100% of   16.76MiB in 00:00:01 at 10.10MiB/s                 \n",
      "[download] Destination: youtube\\KEEMOKAZI\\Saying NO to my spoiled sisters .mp4.f251.webm\n",
      "[download] 100% of  574.81KiB in 00:00:00 at 3.21MiB/s     \n",
      "[Merger] Merging formats into \"youtube\\KEEMOKAZI\\Saying NO to my spoiled sisters .mp4.webm\"\n",
      "Deleting original file youtube\\KEEMOKAZI\\Saying NO to my spoiled sisters .mp4.f616.mp4 (pass -k to keep)\n",
      "Deleting original file youtube\\KEEMOKAZI\\Saying NO to my spoiled sisters .mp4.f251.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\KEEMOKAZI\\metadata\\Saying NO to my spoiled sisters .csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Loop through each data item\n",
    "for data in all:\n",
    "    # Create the 'tiktok' directory if it doesn't exist\n",
    "    tiktok_dir = 'youtube'\n",
    "    if not os.path.exists(tiktok_dir):\n",
    "        os.mkdir(tiktok_dir)\n",
    "\n",
    "    # Create a folder based on the account name under the 'tiktok' directory\n",
    "    account_folder = os.path.join(tiktok_dir, data['channle_name'])\n",
    "    if not os.path.exists(account_folder):\n",
    "        os.mkdir(account_folder)\n",
    "\n",
    "    # Create a 'metadata' folder under the 'account_folder' if it doesn't exist\n",
    "    metadata_folder = os.path.join(account_folder, 'metadata')\n",
    "    if not os.path.exists(metadata_folder):\n",
    "        os.mkdir(metadata_folder)\n",
    "\n",
    "    # Extract the file name from the URL (assuming it ends with .mp4)\n",
    "    # Sanitize the title to remove invalid characters and limit the length\n",
    "    sanitized_title = \"\".join([c for c in data['title'] if c.isalnum() or c in [' ', '-', '_']])[:140]\n",
    "    file_name = sanitized_title + '.mp4'\n",
    "    from yt_dlp import YoutubeDL\n",
    "    video_path = os.path.join(account_folder, file_name)\n",
    "    video_url = data['link']\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo+bestaudio',\n",
    "        'outtmpl': f'{video_path}',\n",
    "    }\n",
    "    \n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([video_url])\n",
    "\n",
    "    # Create a Pandas DataFrame from the data\n",
    "    df = pd.DataFrame([data])\n",
    "\n",
    "    # Save the DataFrame to a CSV file in the 'metadata' folder with the title as the file name\n",
    "    csv_file_path = os.path.join(metadata_folder, f\"{sanitized_title}.csv\")\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Metadata saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03468b9-64df-46e7-9928-6083abf85884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Loop through each data item\n",
    "for data in all:\n",
    "    # Create the 'tiktok' directory if it doesn't exist\n",
    "    tiktok_dir = 'youtube'\n",
    "    if not os.path.exists(tiktok_dir):\n",
    "        os.mkdir(tiktok_dir)\n",
    "\n",
    "    # Create a folder based on the account name under the 'tiktok' directory\n",
    "    account_folder = os.path.join(tiktok_dir, data['channle_name'])\n",
    "    if not os.path.exists(account_folder):\n",
    "        os.mkdir(account_folder)\n",
    "\n",
    "    # Create a 'metadata' folder under the 'account_folder' if it doesn't exist\n",
    "    metadata_folder = os.path.join(account_folder, 'metadata')\n",
    "    if not os.path.exists(metadata_folder):\n",
    "        os.mkdir(metadata_folder)\n",
    "\n",
    "    # Extract the file name from the URL (assuming it ends with .mp4)\n",
    "    # Sanitize the title to remove invalid characters and limit the length\n",
    "    sanitized_title = \"\".join([c for c in data['title'] if c.isalnum() or c in [' ', '-', '_']])[:140]\n",
    "    file_name = sanitized_title + '.mp4'\n",
    "    from yt_dlp import YoutubeDL\n",
    "    video_path = os.path.join(account_folder, file_name)\n",
    "    video_url = data['link']\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo+bestaudio',\n",
    "        'outtmpl': f'{video_path}',\n",
    "    }\n",
    "    \n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([video_url])\n",
    "\n",
    "    # Create a Pandas DataFrame from the data\n",
    "    df = pd.DataFrame([data])\n",
    "\n",
    "    # Save the DataFrame to a CSV file in the 'metadata' folder with the title as the file name\n",
    "    csv_file_path = os.path.join(metadata_folder, f\"{sanitized_title}.csv\")\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Metadata saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f88bb-39ef-47d8-b442-3b09e374e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapetube\n",
    "import yt_dlp\n",
    "\n",
    "\n",
    "\n",
    "channel_id = \"UCzgxx_DM2Dcb9Y1spb9mUJA\"\n",
    "videos = scrapetube.get_channel(channel_id)\n",
    "channel_name = 'Twice'\n",
    "# Initialize an empty list to store video data\n",
    "video_data = []\n",
    "# Create a yt-dlp object\n",
    "ydl = yt_dlp.YoutubeDL()\n",
    "# Initialize a counter to limit the number of videos\n",
    "count = 0\n",
    "for video in videos:\n",
    "    if count < 2:  # Limit to the first two videos\n",
    "\n",
    "        video_id = video['videoId']\n",
    "        date = video['publishedTimeText']\n",
    "        video_links = f\"https://www.youtube.com/watch?v={id}\"\n",
    "        # Extract video information\n",
    "        info_dict = ydl.extract_info(video_links, download=False)\n",
    "    \n",
    "        # Extract the desired information\n",
    "        title = info_dict.get('title', '')\n",
    "        description = info_dict.get('description', '')\n",
    "        views = info_dict.get('view_count', 0)\n",
    "        likes = info_dict.get('like_count', 0)\n",
    "\n",
    "\n",
    "        \n",
    "        import json\n",
    "        \n",
    "        # Define the filename where your JSON data is stored\n",
    "        json_filename = 'video_id.json'\n",
    "        \n",
    "        # Step 1: Open the JSON file and load its contents into a Python dictionary\n",
    "        with open(json_filename, 'r') as json_file:\n",
    "            j_data = json.load(json_file)\n",
    "        \n",
    "        # Step 2: Update the 'video_id' key with a list of video IDs\n",
    "        if 'video_id' not in j_data:\n",
    "            j_data['video_id'] = []  # Initialize an empty list if 'video_id' doesn't exist\n",
    "        \n",
    "        # Add video IDs to the 'video_id' list if they don't already exist\n",
    "        video_ids_to_add = [video_id]\n",
    "        \n",
    "        for video_id in video_ids_to_add:\n",
    "            if video_id not in j_data['video_id']:\n",
    "                j_data['video_id'].append(video_id)\n",
    "        \n",
    "        # Step 3: Save the updated dictionary back to the JSON file\n",
    "        with open(json_filename, 'w') as json_file:\n",
    "            json.dump(j_data, json_file)\n",
    "        \n",
    "        print(f\"Updated JSON data saved to {json_filename}\")\n",
    "\n",
    "\n",
    "        all = {\n",
    "            'channle_name': channel_name,\n",
    "            'channle_id':channel_id,\n",
    "            'title':title,\n",
    "            'description':description,\n",
    "            'view':views,\n",
    "            'like':likes,\n",
    "            'published_date':date,\n",
    "            'link':video_links,\n",
    "            'video_id':video_id\n",
    "            \n",
    "        }\n",
    "        video_data.append(all)\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Now you have video information stored in the video_data list\n",
    "for video in video_data:\n",
    "    print(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdebd0e2-675f-41e6-8012-7c687b4bf77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420a7ac-6466-40a0-87a4-f2b134f77fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e044c-60ce-4060-8588-9ad363f202dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c9c9b-b73f-438f-83e1-f7a2a91d04b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a3df4c-fb53-4bd5-b18d-648c81f003c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
