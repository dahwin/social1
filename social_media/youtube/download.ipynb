{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab3bde2-7994-45b9-9956-36f8649f96e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=L5kIXRdwGAI\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=79rSUeJr1sE\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=l1uE_pBqnvE\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=y1sF6ZeASU0\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=aRBVzMluVnU\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=B5Ch3jk2u0s\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=nZxr3vngv88\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=2axQXJK-6R0\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=AM2-3ufgTCs\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=4Xm2jWeuKMc\n",
      "[{'channle_name': 'MrBeast', 'channle_id': 'UCX6OQ3DkcsbYNE6H8uQQuVA', 'title': 'Katana Vs Bullet', 'description': '', 'view': '133426302', 'like': '9111017', 'publish_data': '2023-09-07T17:00:01Z', 'link': 'https://www.youtube.com/watch?v=L5kIXRdwGAI', 'video_id': 'L5kIXRdwGAI'}, {'channle_name': 'Dylan Anderson', 'channle_id': 'UCpuT8AlP9P9EgW_pZ0_xInQ', 'title': 'This guy had his coworkers fooled üòÇ', 'description': '', 'view': '16381008', 'like': '983767', 'publish_data': '2023-09-10T01:46:27Z', 'link': 'https://www.youtube.com/watch?v=79rSUeJr1sE', 'video_id': '79rSUeJr1sE'}, {'channle_name': 'Ryan HD', 'channle_id': 'UC68zG933W7D9V9H9TLXBPZQ', 'title': 'Students Excuses For Not Doing Their Homework.', 'description': '#school #student #relatable #teacher #studentlife #studentmemes', 'view': '30651574', 'like': '1940272', 'publish_data': '2023-09-07T14:50:47Z', 'link': 'https://www.youtube.com/watch?v=l1uE_pBqnvE', 'video_id': 'l1uE_pBqnvE'}, {'channle_name': 'Vivziepop', 'channle_id': 'UCzfyYtgvkx5mLy8nlLlayYg', 'title': 'HELLUVA BOSS - OOPS // S2: Episode 6', 'description': 'Fizz goes to buy milk, and things sure do happen.\\nWARNING: for sad clowns.\\nHELLUVA BOSS MERCH AVAILABLE ‚ñ∫ https://sharkrobot.com/vivziepop\\n\\nWANT MORE? WATCH OUR OTHER EPISODES!\\n\\nPilot Episode ‚ñ∫ https://www.youtube.com/watch?v=OlahNrlcgS4\\nS1 Episode One ‚ñ∫ https://www.youtube.com/watch?v=el_PChGfJN8\\nS1 Episode Two ‚ñ∫ https://www.youtube.com/watch?v=kpnwRg268FQ\\nS1 Episode Three ‚ñ∫  https://www.youtube.com/watch?v=RghsgkZKedg\\nS1 Episode Four ‚ñ∫ https://www.youtube.com/watch?v=1ZFseYPmkAk\\nS1 Episode Five ‚ñ∫ https://www.youtube.com/watch?v=h2ZmVAdezF8\\nS1 Episode Six ‚ñ∫ https://youtu.be/yXErLiSbxXQ\\nS1 Episode Seven ‚ñ∫ https://youtu.be/8zyGQquL8VM\\nS2 Episode One ‚ñ∫ https://youtu.be/_spuxXnul0U\\nS2 Episode Two ‚ñ∫ https://youtu.be/4J0xFUyz1nw\\nS2 Episode Three ‚ñ∫ https://youtu.be/j1BfO7VlIw4\\nS2 Episode Four ‚ñ∫ https://youtu.be/KJy7T24rhg0\\nS2 Episode Five ‚ñ∫ https://youtu.be/lYu1ysDULwA\\n\\nOr Support on Patreon!\\nhttp://www.patreon.com/VivienneMedrano\\n\\nThumbnail by https://twitter.com/GasuGuma\\n\\n#HelluvaBoss #HelluvaBossSeason2 #SEASONTWO', 'view': '8941084', 'like': '729146', 'publish_data': '2023-09-09T16:00:51Z', 'link': 'https://www.youtube.com/watch?v=y1sF6ZeASU0', 'video_id': 'y1sF6ZeASU0'}, {'channle_name': 'Sleazy Pete', 'channle_id': 'UCsoGdClWwWeAXieOTt7sB-g', 'title': 'Mattie Finally Tracks Down Chaney! (1/3) ‚Ä¢ (13/18)‚îÉTrue Grit (2010)‚îÉ4K‚îÉ#sleazypete', 'description': '', 'view': '4015437', 'like': '188429', 'publish_data': '2023-09-10T19:44:23Z', 'link': 'https://www.youtube.com/watch?v=aRBVzMluVnU', 'video_id': 'aRBVzMluVnU'}, {'channle_name': 'Daily Dose Of Internet', 'channle_id': 'UCdC0An4ZPNr_YiFiYoVbwaw', 'title': \"Sleepwalker Won't Stop Staring\", 'description': 'Source: Collab.Inc\\n\\n#shorts', 'view': '8171422', 'like': '465958', 'publish_data': '2023-09-10T19:38:16Z', 'link': 'https://www.youtube.com/watch?v=B5Ch3jk2u0s', 'video_id': 'B5Ch3jk2u0s'}, {'channle_name': 'Ray William Johnson', 'channle_id': 'UCGt7X90Au6BV8rf49BiM6Dg', 'title': 'I can‚Äôt handle this drama', 'description': 'TikTok star claims she was denied entry by Deep Hollow Horse Ranch because of her weight \\n\\n#remibader #horse #horsebackriding #ranch #raywilliamjohnson #shorts', 'view': '9546488', 'like': '973803', 'publish_data': '2023-09-08T00:16:40Z', 'link': 'https://www.youtube.com/watch?v=nZxr3vngv88', 'video_id': 'nZxr3vngv88'}, {'channle_name': 'Cop Humor', 'channle_id': 'UCW2R_zesOCCKqkNj4lJZ77A', 'title': 'Man Uses Donuts to Fish for Cops - Hilarious Reaction! üòÇ #shorts', 'description': \"When this man (therealernest425 TT) noticed a police car parked nearby, he decided to have a bit of fun by fishing for cops, using a bag of donuts! But he never expected one of the police officers to react the way he did! Don't miss this hilarious police reaction caught on camera!\\n\\nPlease like, share, and subscribe for more hilarious videos! Thank you üòä\", 'view': '9861022', 'like': '493857', 'publish_data': '2023-09-08T19:17:32Z', 'link': 'https://www.youtube.com/watch?v=2axQXJK-6R0', 'video_id': '2axQXJK-6R0'}, {'channle_name': 'Worldwide Watchlist Movies', 'channle_id': 'UCSMZPYKUC1WXl5KseRBI9yA', 'title': 'Nolan visits elders about late rent üòØ', 'description': '#therookie #shorts', 'view': '2107970', 'like': '216864', 'publish_data': '2023-09-13T15:20:41Z', 'link': 'https://www.youtube.com/watch?v=AM2-3ufgTCs', 'video_id': 'AM2-3ufgTCs'}, {'channle_name': 'KEEMOKAZI', 'channle_id': 'UCMOLiZpKXp-w5CNfqJObQUw', 'title': 'Saying ‚ÄúNO‚Äù to my spoiled sisters üòÇ', 'description': '', 'view': '8085483', 'like': '497943', 'publish_data': '2023-09-10T22:38:32Z', 'link': 'https://www.youtube.com/watch?v=4Xm2jWeuKMc', 'video_id': '4Xm2jWeuKMc'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from IPython.display import JSON\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n",
    "\n",
    "# Disable OAuthlib's HTTPS verification when running locally.\n",
    "# *DO NOT* leave this option enabled in production.\n",
    "os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "api_key = \"AIzaSyBbxGwSTdF9VFHcHyarpxOAyFZ-tdCp_6o\"\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=api_key)\n",
    "\n",
    "request = youtube.videos().list(\n",
    "    part=\"snippet,contentDetails,statistics\",\n",
    "    chart=\"mostPopular\",\n",
    "    regionCode=\"US\",\n",
    "    maxResults=10,  # Set the number of results per page\n",
    "    videoCategoryId=\"24\"  # Set the video category to \"Shorts\"\n",
    ")\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "data = []\n",
    "for item in response['items']:\n",
    "    video_id = item[\"id\"]\n",
    "    video_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "    channle_name = item['snippet']['channelTitle']\n",
    "    channle_id = item['snippet']['channelId']\n",
    "    title = item['snippet']['localized']['title']\n",
    "    description = item['snippet']['localized']['description']\n",
    "    publish_data = item['snippet']['publishedAt']\n",
    "    view = item['statistics']['viewCount']\n",
    "    like = item['statistics'].get('likeCount', 0)  # Handle missing 'likeCount'\n",
    "\n",
    "    import json\n",
    "    \n",
    "    # Define the filename where your JSON data is stored\n",
    "    json_filename = 'video_id.json'\n",
    "    \n",
    "    # Step 1: Open the JSON file and load its contents into a Python dictionary\n",
    "    with open(json_filename, 'r') as json_file:\n",
    "        j_data = json.load(json_file)\n",
    "    \n",
    "    # Step 2: Update the 'video_id' key with a list of video IDs\n",
    "    if 'video_id' not in j_data:\n",
    "        j_data['video_id'] = []  # Initialize an empty list if 'video_id' doesn't exist\n",
    "    \n",
    "    # Add video IDs to the 'video_id' list if they don't already exist\n",
    "    video_ids_to_add = [video_id]\n",
    "    \n",
    "    for video_id in video_ids_to_add:\n",
    "        if video_id not in j_data['video_id']:\n",
    "            j_data['video_id'].append(video_id)\n",
    "    \n",
    "    # Step 3: Save the updated dictionary back to the JSON file\n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json.dump(j_data, json_file)\n",
    "    \n",
    "    print(f\"Updated JSON data saved to {json_filename}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    all_data = {\n",
    "        'channle_name':channle_name,\n",
    "        'channle_id':channle_id,\n",
    "        'title': title,\n",
    "        'description': description,\n",
    "        'view': view,\n",
    "        'like': like,\n",
    "        'publish_data': publish_data,\n",
    "        'link':video_link,\n",
    "        'video_id':video_id\n",
    "    }\n",
    "    data.append(all_data)\n",
    "    print(video_link)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c63649-423e-49a1-8c8e-0e9bbbaaf287",
   "metadata": {},
   "source": [
    "# Perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b57acd-8e86-4cfd-ba60-69b902f088b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON data saved to video_id.json\n",
      "Updated JSON data saved to video_id.json\n",
      "Updated JSON data saved to video_id.json\n",
      "Updated JSON data saved to video_id.json\n",
      "Updated JSON data saved to video_id.json\n",
      "Updated JSON data saved to video_id.json\n",
      "Updated JSON data saved to video_id.json\n",
      "Updated JSON data saved to video_id.json\n",
      "Updated JSON data saved to video_id.json\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=uUsUVBZyqls\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=bENoOL2UkEw\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=5ZWbSpZ1xQc\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=5RhAkIUERyQ\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=N5uq_go0eBs\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=P7xBv3Ql6Kk\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=sEfSfIhrKg4\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=7XB8wJiyMyY\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=clNhAEL1eOI\n",
      "Updated JSON data saved to video_id.json\n",
      "https://www.youtube.com/watch?v=WNW4dw775ec\n",
      "Updated JSON data saved to video_id.json\n",
      "[{'channle_name': 'White Dolemite', 'channle_id': 'UCvyUSK2tnMf9PFNH5PZIwtg', 'title': 'This might be my last time doing this prank üò≥üòÇü§¶üèΩ\\u200d‚ôÇÔ∏è #prank #ytshorts #shorts #viral', 'description': '', 'view': '1699651', 'like': 0, 'publish_data': '2023-09-10T22:58:28Z', 'link': 'https://www.youtube.com/watch?v=uUsUVBZyqls', 'video_id': 'uUsUVBZyqls'}, {'channle_name': 'CineLifeAI', 'channle_id': 'UCrMkun7kxG4XYh-mvedU1xQ', 'title': 'Part 2 #foryou', 'description': '', 'view': '2328691', 'like': '189904', 'publish_data': '2023-09-11T17:55:24Z', 'link': 'https://www.youtube.com/watch?v=bENoOL2UkEw', 'video_id': 'bENoOL2UkEw'}, {'channle_name': 'Kai Cenat Live', 'channle_id': 'UCvCfpQXRXdJdL07pzTIA6Cw', 'title': 'Speed is BUGGING', 'description': 'Speed is BUGGING\\nFOLLOW ME ON TWITCH: https://www.twitch.tv/kaicenat\\n\\n üî¥Follow My Socials:\\nMain Channel: https://youtube.com/c/KaiCenat\\nTwitter : https://twitter.com/KaiCenat\\nInstagram: https://www.instagram.com/kaicenat\\n#KaiCenat #KaiCenatLive #KaiCenatStream', 'view': '1847201', 'like': '135670', 'publish_data': '2023-09-12T22:21:15Z', 'link': 'https://www.youtube.com/watch?v=5ZWbSpZ1xQc', 'video_id': '5ZWbSpZ1xQc'}, {'channle_name': 'Big guy', 'channle_id': 'UCWZp4y1jqBuvLtiyxSs_ZBw', 'title': 'Schlatt Finds His Manliest Viewer', 'description': 'jschlatt finds his manliest viewer while schlatt is ranking your cursed cooking.\\n#jschlatt #schlatt #bigguy #shorts', 'view': '3204241', 'like': '246810', 'publish_data': '2023-09-11T16:30:01Z', 'link': 'https://www.youtube.com/watch?v=5RhAkIUERyQ', 'video_id': '5RhAkIUERyQ'}, {'channle_name': 'MTV', 'channle_id': 'UCxAICW_LdkfFYwTqTHHE0vg', 'title': '*NSYNC Presents Taylor Swift w/ Best Pop Award | 2023 VMAs', 'description': '*NSYNC presents Taylor Swift with the Best Pop Award at the 2023 MTV Video Music Awards. Check out more from the 2023 VMAs here: http://www.mtv.com/vma\\n\\nParamount+ is here! Stream all your favorite shows now on Paramount+. Try it FREE at https://bit.ly/3qyOeOf\\n\\n#VMAs #MTV #NSYNC #TaylorSwift \\n\\nSubscribe to MTV: http://goo.gl/NThuhC\\n\\nMore from MTV:\\nOfficial MTV Website: http://www.mtv.com/\\nLike MTV: https://www.facebook.com/MTV\\nFollow MTV: https://twitter.com/MTV\\nMTV Instagram: http://instagram.com/mtv\\n\\n#MTV is your destination for the hit series WNO, VMA, Jersey Shore, The Challenge, MTV Floribama Shore, Teen Mom and much more!', 'view': '2551749', 'like': '69973', 'publish_data': '2023-09-13T01:39:23Z', 'link': 'https://www.youtube.com/watch?v=N5uq_go0eBs', 'video_id': 'N5uq_go0eBs'}, {'channle_name': 'Finest Trends', 'channle_id': 'UCaX6tz4DvYv_a1qKgoMZshA', 'title': 'You got a second?', 'description': 'You got a second? Crazy lady karen is on a mission \\n\\n#shorts #crazylady', 'view': '17052057', 'like': '981681', 'publish_data': '2023-09-04T00:00:10Z', 'link': 'https://www.youtube.com/watch?v=P7xBv3Ql6Kk', 'video_id': 'P7xBv3Ql6Kk'}, {'channle_name': 'Andrea & Lewis', 'channle_id': 'UCIvjHBJvS4koTix_Z1S15Aw', 'title': 'Puerto Rican Grandma Pranks Irish Man', 'description': '', 'view': '12404829', 'like': '835857', 'publish_data': '2023-09-04T20:14:46Z', 'link': 'https://www.youtube.com/watch?v=sEfSfIhrKg4', 'video_id': 'sEfSfIhrKg4'}, {'channle_name': 'jacobweeby', 'channle_id': 'UCRFSF9WS_g0wqm32kjJ-jNw', 'title': 'Streamer House (Impressions) #shorts', 'description': 'Bruh üíÄ \\n\\n#xqc #moistcr1tikal #jynxzi #anime #fyp #funny #shorts', 'view': '4923937', 'like': '289188', 'publish_data': '2023-09-10T15:00:05Z', 'link': 'https://www.youtube.com/watch?v=7XB8wJiyMyY', 'video_id': '7XB8wJiyMyY'}, {'channle_name': 'Sambucha', 'channle_id': 'UCWBWgCD4oAqT3hUeq40SCUw', 'title': 'Is It Okay To Bring ____ To These Places?', 'description': 'Get Merch:\\nhttps://bucha.shop\\n\\nBecome a Member:\\nhttps://www.youtube.com/channel/UCWBWgCD4oAqT3hUeq40SCUw/join\\n\\nFollow me here:\\nInstagram ‚ñ∫ https://www.instagram.com/sambuchalul\\nTwitter ‚ñ∫ https://www.twitter.com/sambucha\\n\\n#shorts #church #funeral #food #phone #behavior #public #howto #movies #restaurant #sambucha', 'view': '2045749', 'like': '196599', 'publish_data': '2023-09-14T17:15:01Z', 'link': 'https://www.youtube.com/watch?v=clNhAEL1eOI', 'video_id': 'clNhAEL1eOI'}, {'channle_name': 'Law By Mike', 'channle_id': 'UCKmmERguliWTynG9OIoDhDw', 'title': 'Cops Can Get Your DNA!? #law #education', 'description': \"Don't Make These Mistakes With The Police! Subscribe to @LawByMike for more tips!\\n\\n‚≠ê Become a member of THE INNER CIRCLE to get exclusive perks‚≠ê\\n*link in bio*\\n\\n‚öñÔ∏è Questions? Issues?üëá \\n*link in bio*\\n\\n#police #hotels #lawyer #lawbymike #repeatafterme #tiktok #youtubeshorts #cop #cops #lawyers #legal #attorney #lawschool #legal\\n\\nDISCLAIMER (Of course, I'd have one üòÅ)\\nHey, you might think that this info makes me your lawyer, but it doesn‚Äôt and I‚Äôm not. Sorry, but I AM NOT YOUR LAWYER unless we have an engagement agreement. I am just providing public information here, like a library does, and am not providing you with legal advice about your situation. So, it would be totally unreasonable for you to conclude we have an attorney-client relationship just because you're viewing this information.\", 'view': '1356519', 'like': '177084', 'publish_data': '2023-09-13T14:45:01Z', 'link': 'https://www.youtube.com/watch?v=WNW4dw775ec', 'video_id': 'WNW4dw775ec'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from IPython.display import JSON\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n",
    "\n",
    "# Disable OAuthlib's HTTPS verification when running locally.\n",
    "# *DO NOT* leave this option enabled in production.\n",
    "os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "api_key = \"AIzaSyBbxGwSTdF9VFHcHyarpxOAyFZ-tdCp_6o\"\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=api_key)\n",
    "\n",
    "request = youtube.videos().list(\n",
    "    part=\"snippet,contentDetails,statistics\",\n",
    "    chart=\"mostPopular\",\n",
    "    regionCode=\"US\",\n",
    "    maxResults=20,  # Set the number of results per page\n",
    "    videoCategoryId=\"24\"  # Set the video category to \"Shorts\"\n",
    ")\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "data = []\n",
    "for item in response['items']:\n",
    "    video_id = item[\"id\"]\n",
    "    video_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "    channle_name = item['snippet']['channelTitle']\n",
    "    channle_id = item['snippet']['channelId']\n",
    "    title = item['snippet']['localized']['title']\n",
    "    description = item['snippet']['localized']['description']\n",
    "    publish_data = item['snippet']['publishedAt']\n",
    "    view = item['statistics']['viewCount']\n",
    "    like = item['statistics'].get('likeCount', 0)  # Handle missing 'likeCount'\n",
    "\n",
    "    import json\n",
    "    \n",
    "    # Define the filename where your JSON data is stored\n",
    "    json_filename = 'video_id.json'\n",
    "    \n",
    "    # Step 1: Open the JSON file and load its contents into a Python dictionary\n",
    "    with open(json_filename, 'r') as json_file:\n",
    "        j_data = json.load(json_file)\n",
    "    \n",
    "    # Step 2: Update the 'video_id' key with a list of video IDs\n",
    "    if 'video_id' not in j_data:\n",
    "        j_data['video_id'] = []  # Initialize an empty list if 'video_id' doesn't exist\n",
    "    \n",
    "    # Add video IDs to the 'video_id' list if they don't already exist\n",
    "    video_ids_to_add = [video_id]\n",
    "    \n",
    "    for video_id in video_ids_to_add:\n",
    "        if video_id not in j_data['video_id']:\n",
    "            j_data['video_id'].append(video_id)\n",
    "\n",
    "            all_data = {\n",
    "                'channle_name':channle_name,\n",
    "                'channle_id':channle_id,\n",
    "                'title': title,\n",
    "                'description': description,\n",
    "                'view': view,\n",
    "                'like': like,\n",
    "                'publish_data': publish_data,\n",
    "                'link':video_link,\n",
    "                'video_id':video_id\n",
    "            }\n",
    "            data.append(all_data)\n",
    "            print(video_link)\n",
    "    # Step 3: Save the updated dictionary back to the JSON file\n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json.dump(j_data, json_file)\n",
    "    \n",
    "    print(f\"Updated JSON data saved to {json_filename}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaba5df4-e775-4320-a30d-260ae1254c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "151a7cbf-1824-41bf-906a-35fe50e98c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a5d3cfc-0574-44f8-96bc-d2b2458b37f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=uUsUVBZyqls\n",
      "[youtube] uUsUVBZyqls: Downloading webpage\n",
      "[youtube] uUsUVBZyqls: Downloading ios player API JSON\n",
      "[youtube] uUsUVBZyqls: Downloading android player API JSON\n",
      "[youtube] uUsUVBZyqls: Downloading m3u8 information\n",
      "[info] uUsUVBZyqls: Downloading 1 format(s): 616+251\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 3\n",
      "[download] Destination: youtube\\White Dolemite\\This might be my last time doing this prank  prank ytshorts shorts viral.mp4.f616.mp4\n",
      "[download] 100% of    8.72MiB in 00:00:02 at 3.14MiB/s                  \n",
      "[download] Destination: youtube\\White Dolemite\\This might be my last time doing this prank  prank ytshorts shorts viral.mp4.f251.webm\n",
      "[download] 100% of  226.19KiB in 00:00:00 at 579.86KiB/s \n",
      "[Merger] Merging formats into \"youtube\\White Dolemite\\This might be my last time doing this prank  prank ytshorts shorts viral.mp4.webm\"\n",
      "Deleting original file youtube\\White Dolemite\\This might be my last time doing this prank  prank ytshorts shorts viral.mp4.f616.mp4 (pass -k to keep)\n",
      "Deleting original file youtube\\White Dolemite\\This might be my last time doing this prank  prank ytshorts shorts viral.mp4.f251.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\White Dolemite\\metadata\\This might be my last time doing this prank  prank ytshorts shorts viral.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=bENoOL2UkEw\n",
      "[youtube] bENoOL2UkEw: Downloading webpage\n",
      "[youtube] bENoOL2UkEw: Downloading ios player API JSON\n",
      "[youtube] bENoOL2UkEw: Downloading android player API JSON\n",
      "[youtube] bENoOL2UkEw: Downloading m3u8 information\n",
      "[info] bENoOL2UkEw: Downloading 1 format(s): 247+251\n",
      "[download] Destination: youtube\\CineLifeAI\\Part 2 foryou.mp4.f247.webm\n",
      "[download] 100% of    2.30MiB in 00:00:00 at 7.57MiB/s   \n",
      "[download] Destination: youtube\\CineLifeAI\\Part 2 foryou.mp4.f251.webm\n",
      "[download] 100% of  676.33KiB in 00:00:00 at 5.66MiB/s     \n",
      "[Merger] Merging formats into \"youtube\\CineLifeAI\\Part 2 foryou.mp4.webm\"\n",
      "Deleting original file youtube\\CineLifeAI\\Part 2 foryou.mp4.f247.webm (pass -k to keep)\n",
      "Deleting original file youtube\\CineLifeAI\\Part 2 foryou.mp4.f251.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\CineLifeAI\\metadata\\Part 2 foryou.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=5ZWbSpZ1xQc\n",
      "[youtube] 5ZWbSpZ1xQc: Downloading webpage\n",
      "[youtube] 5ZWbSpZ1xQc: Downloading ios player API JSON\n",
      "[youtube] 5ZWbSpZ1xQc: Downloading android player API JSON\n",
      "[youtube] 5ZWbSpZ1xQc: Downloading m3u8 information\n",
      "[info] 5ZWbSpZ1xQc: Downloading 1 format(s): 303+251\n",
      "[download] Destination: youtube\\Kai Cenat Live\\Speed is BUGGING.mp4.f303.webm\n",
      "[download] 100% of    9.24MiB in 00:00:00 at 13.29MiB/s  \n",
      "[download] Destination: youtube\\Kai Cenat Live\\Speed is BUGGING.mp4.f251.webm\n",
      "[download] 100% of  251.16KiB in 00:00:00 at 2.85MiB/s   \n",
      "[Merger] Merging formats into \"youtube\\Kai Cenat Live\\Speed is BUGGING.mp4.webm\"\n",
      "Deleting original file youtube\\Kai Cenat Live\\Speed is BUGGING.mp4.f251.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Kai Cenat Live\\Speed is BUGGING.mp4.f303.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\Kai Cenat Live\\metadata\\Speed is BUGGING.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=5RhAkIUERyQ\n",
      "[youtube] 5RhAkIUERyQ: Downloading webpage\n",
      "[youtube] 5RhAkIUERyQ: Downloading ios player API JSON\n",
      "[youtube] 5RhAkIUERyQ: Downloading android player API JSON\n",
      "[youtube] 5RhAkIUERyQ: Downloading m3u8 information\n",
      "[info] 5RhAkIUERyQ: Downloading 1 format(s): 303+251\n",
      "[download] Destination: youtube\\Big guy\\Schlatt Finds His Manliest Viewer.mp4.f303.webm\n",
      "[download] 100% of    8.20MiB in 00:00:00 at 18.16MiB/s  \n",
      "[download] Destination: youtube\\Big guy\\Schlatt Finds His Manliest Viewer.mp4.f251.webm\n",
      "[download] 100% of  494.97KiB in 00:00:00 at 2.63MiB/s   \n",
      "[Merger] Merging formats into \"youtube\\Big guy\\Schlatt Finds His Manliest Viewer.mp4.webm\"\n",
      "Deleting original file youtube\\Big guy\\Schlatt Finds His Manliest Viewer.mp4.f251.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Big guy\\Schlatt Finds His Manliest Viewer.mp4.f303.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\Big guy\\metadata\\Schlatt Finds His Manliest Viewer.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=N5uq_go0eBs\n",
      "[youtube] N5uq_go0eBs: Downloading webpage\n",
      "[youtube] N5uq_go0eBs: Downloading ios player API JSON\n",
      "[youtube] N5uq_go0eBs: Downloading android player API JSON\n",
      "[youtube] N5uq_go0eBs: Downloading m3u8 information\n",
      "[info] N5uq_go0eBs: Downloading 1 format(s): 248+251\n",
      "[download] Destination: youtube\\MTV\\NSYNC Presents Taylor Swift w Best Pop Award  2023 VMAs.mp4.f248.webm\n",
      "[download] 100% of   61.12MiB in 00:00:01 at 31.87MiB/s    \n",
      "[download] Destination: youtube\\MTV\\NSYNC Presents Taylor Swift w Best Pop Award  2023 VMAs.mp4.f251.webm\n",
      "[download] 100% of    4.24MiB in 00:00:00 at 19.06MiB/s  \n",
      "[Merger] Merging formats into \"youtube\\MTV\\NSYNC Presents Taylor Swift w Best Pop Award  2023 VMAs.mp4.webm\"\n",
      "Deleting original file youtube\\MTV\\NSYNC Presents Taylor Swift w Best Pop Award  2023 VMAs.mp4.f248.webm (pass -k to keep)\n",
      "Deleting original file youtube\\MTV\\NSYNC Presents Taylor Swift w Best Pop Award  2023 VMAs.mp4.f251.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\MTV\\metadata\\NSYNC Presents Taylor Swift w Best Pop Award  2023 VMAs.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=P7xBv3Ql6Kk\n",
      "[youtube] P7xBv3Ql6Kk: Downloading webpage\n",
      "[youtube] P7xBv3Ql6Kk: Downloading ios player API JSON\n",
      "[youtube] P7xBv3Ql6Kk: Downloading android player API JSON\n",
      "[youtube] P7xBv3Ql6Kk: Downloading m3u8 information\n",
      "[info] P7xBv3Ql6Kk: Downloading 1 format(s): 616+251\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 6\n",
      "[download] Destination: youtube\\Finest Trends\\You got a second.mp4.f616.mp4\n",
      "[download] 100% of   10.84MiB in 00:00:00 at 11.02MiB/s                 \n",
      "[download] Destination: youtube\\Finest Trends\\You got a second.mp4.f251.webm\n",
      "[download] 100% of  537.61KiB in 00:00:00 at 6.51MiB/s     \n",
      "[Merger] Merging formats into \"youtube\\Finest Trends\\You got a second.mp4.webm\"\n",
      "Deleting original file youtube\\Finest Trends\\You got a second.mp4.f251.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Finest Trends\\You got a second.mp4.f616.mp4 (pass -k to keep)\n",
      "Metadata saved to youtube\\Finest Trends\\metadata\\You got a second.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=sEfSfIhrKg4\n",
      "[youtube] sEfSfIhrKg4: Downloading webpage\n",
      "[youtube] sEfSfIhrKg4: Downloading ios player API JSON\n",
      "[youtube] sEfSfIhrKg4: Downloading android player API JSON\n",
      "[youtube] sEfSfIhrKg4: Downloading m3u8 information\n",
      "[info] sEfSfIhrKg4: Downloading 1 format(s): 247+251\n",
      "[download] Destination: youtube\\Andrea & Lewis\\Puerto Rican Grandma Pranks Irish Man.mp4.f247.webm\n",
      "[download] 100% of    2.26MiB in 00:00:00 at 13.61MiB/s  \n",
      "[download] Destination: youtube\\Andrea & Lewis\\Puerto Rican Grandma Pranks Irish Man.mp4.f251.webm\n",
      "[download] 100% of  287.26KiB in 00:00:00 at 2.68MiB/s   \n",
      "[Merger] Merging formats into \"youtube\\Andrea & Lewis\\Puerto Rican Grandma Pranks Irish Man.mp4.webm\"\n",
      "Deleting original file youtube\\Andrea & Lewis\\Puerto Rican Grandma Pranks Irish Man.mp4.f247.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Andrea & Lewis\\Puerto Rican Grandma Pranks Irish Man.mp4.f251.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\Andrea & Lewis\\metadata\\Puerto Rican Grandma Pranks Irish Man.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=7XB8wJiyMyY\n",
      "[youtube] 7XB8wJiyMyY: Downloading webpage\n",
      "[youtube] 7XB8wJiyMyY: Downloading ios player API JSON\n",
      "[youtube] 7XB8wJiyMyY: Downloading android player API JSON\n",
      "[youtube] 7XB8wJiyMyY: Downloading m3u8 information\n",
      "[info] 7XB8wJiyMyY: Downloading 1 format(s): 616+251\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 11\n",
      "[download] Destination: youtube\\jacobweeby\\Streamer House Impressions shorts.mp4.f616.mp4\n",
      "[download] 100% of   24.93MiB in 00:00:21 at 1.14MiB/s                     \n",
      "[download] Destination: youtube\\jacobweeby\\Streamer House Impressions shorts.mp4.f251.webm\n",
      "[download] 100% of  886.06KiB in 00:00:00 at 5.40MiB/s     \n",
      "[Merger] Merging formats into \"youtube\\jacobweeby\\Streamer House Impressions shorts.mp4.webm\"\n",
      "Deleting original file youtube\\jacobweeby\\Streamer House Impressions shorts.mp4.f616.mp4 (pass -k to keep)\n",
      "Deleting original file youtube\\jacobweeby\\Streamer House Impressions shorts.mp4.f251.webm (pass -k to keep)\n",
      "Metadata saved to youtube\\jacobweeby\\metadata\\Streamer House Impressions shorts.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=clNhAEL1eOI\n",
      "[youtube] clNhAEL1eOI: Downloading webpage\n",
      "[youtube] clNhAEL1eOI: Downloading ios player API JSON\n",
      "[youtube] clNhAEL1eOI: Downloading android player API JSON\n",
      "[youtube] clNhAEL1eOI: Downloading m3u8 information\n",
      "[info] clNhAEL1eOI: Downloading 1 format(s): 616+251\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 10\n",
      "[download] Destination: youtube\\Sambucha\\Is It Okay To Bring ____ To These Places.mp4.f616.mp4\n",
      "[download] 100% of   13.41MiB in 00:00:14 at 949.73KiB/s                  \n",
      "[download] Destination: youtube\\Sambucha\\Is It Okay To Bring ____ To These Places.mp4.f251.webm\n",
      "[download] 100% of 1011.87KiB in 00:00:00 at 8.33MiB/s   \n",
      "[Merger] Merging formats into \"youtube\\Sambucha\\Is It Okay To Bring ____ To These Places.mp4.webm\"\n",
      "Deleting original file youtube\\Sambucha\\Is It Okay To Bring ____ To These Places.mp4.f251.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Sambucha\\Is It Okay To Bring ____ To These Places.mp4.f616.mp4 (pass -k to keep)\n",
      "Metadata saved to youtube\\Sambucha\\metadata\\Is It Okay To Bring ____ To These Places.csv\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=WNW4dw775ec\n",
      "[youtube] WNW4dw775ec: Downloading webpage\n",
      "[youtube] WNW4dw775ec: Downloading ios player API JSON\n",
      "[youtube] WNW4dw775ec: Downloading android player API JSON\n",
      "[youtube] WNW4dw775ec: Downloading m3u8 information\n",
      "[info] WNW4dw775ec: Downloading 1 format(s): 616+251\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 11\n",
      "[download] Destination: youtube\\Law By Mike\\Cops Can Get Your DNA law education.mp4.f616.mp4\n",
      "[download] 100% of   28.31MiB in 00:00:02 at 11.97MiB/s                   \n",
      "[download] Destination: youtube\\Law By Mike\\Cops Can Get Your DNA law education.mp4.f251.webm\n",
      "[download] 100% of  834.06KiB in 00:00:00 at 6.70MiB/s   \n",
      "[Merger] Merging formats into \"youtube\\Law By Mike\\Cops Can Get Your DNA law education.mp4.webm\"\n",
      "Deleting original file youtube\\Law By Mike\\Cops Can Get Your DNA law education.mp4.f251.webm (pass -k to keep)\n",
      "Deleting original file youtube\\Law By Mike\\Cops Can Get Your DNA law education.mp4.f616.mp4 (pass -k to keep)\n",
      "Metadata saved to youtube\\Law By Mike\\metadata\\Cops Can Get Your DNA law education.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Loop through each data item\n",
    "for data in all:\n",
    "    # Create the 'tiktok' directory if it doesn't exist\n",
    "    tiktok_dir = 'youtube'\n",
    "    if not os.path.exists(tiktok_dir):\n",
    "        os.mkdir(tiktok_dir)\n",
    "\n",
    "    # Create a folder based on the account name under the 'tiktok' directory\n",
    "    account_folder = os.path.join(tiktok_dir, data['channle_name'])\n",
    "    if not os.path.exists(account_folder):\n",
    "        os.mkdir(account_folder)\n",
    "\n",
    "    # Create a 'metadata' folder under the 'account_folder' if it doesn't exist\n",
    "    metadata_folder = os.path.join(account_folder, 'metadata')\n",
    "    if not os.path.exists(metadata_folder):\n",
    "        os.mkdir(metadata_folder)\n",
    "\n",
    "    # Extract the file name from the URL (assuming it ends with .mp4)\n",
    "    # Sanitize the title to remove invalid characters and limit the length\n",
    "    sanitized_title = \"\".join([c for c in data['title'] if c.isalnum() or c in [' ', '-', '_']])[:140]\n",
    "    file_name = sanitized_title + '.mp4'\n",
    "    from yt_dlp import YoutubeDL\n",
    "    video_path = os.path.join(account_folder, file_name)\n",
    "    video_url = data['link']\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo+bestaudio',\n",
    "        'outtmpl': f'{video_path}',\n",
    "    }\n",
    "    \n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([video_url])\n",
    "\n",
    "    # Create a Pandas DataFrame from the data\n",
    "    df = pd.DataFrame([data])\n",
    "\n",
    "    # Save the DataFrame to a CSV file in the 'metadata' folder with the title as the file name\n",
    "    csv_file_path = os.path.join(metadata_folder, f\"{sanitized_title}.csv\")\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Metadata saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03468b9-64df-46e7-9928-6083abf85884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# # Loop through each data item\n",
    "# for data in all:\n",
    "#     # Create the 'tiktok' directory if it doesn't exist\n",
    "#     tiktok_dir = 'youtube'\n",
    "#     if not os.path.exists(tiktok_dir):\n",
    "#         os.mkdir(tiktok_dir)\n",
    "\n",
    "#     # Create a folder based on the account name under the 'tiktok' directory\n",
    "#     account_folder = os.path.join(tiktok_dir, data['channle_name'])\n",
    "#     if not os.path.exists(account_folder):\n",
    "#         os.mkdir(account_folder)\n",
    "\n",
    "#     # Create a 'metadata' folder under the 'account_folder' if it doesn't exist\n",
    "#     metadata_folder = os.path.join(account_folder, 'metadata')\n",
    "#     if not os.path.exists(metadata_folder):\n",
    "#         os.mkdir(metadata_folder)\n",
    "\n",
    "#     # Extract the file name from the URL (assuming it ends with .mp4)\n",
    "#     # Sanitize the title to remove invalid characters and limit the length\n",
    "#     sanitized_title = \"\".join([c for c in data['title'] if c.isalnum() or c in [' ', '-', '_']])[:140]\n",
    "#     file_name = sanitized_title + '.mp4'\n",
    "#     from yt_dlp import YoutubeDL\n",
    "#     video_path = os.path.join(account_folder, file_name)\n",
    "#     video_url = data['link']\n",
    "    \n",
    "#     ydl_opts = {\n",
    "#         'format': 'bestvideo+bestaudio',\n",
    "#         'outtmpl': f'{video_path}',\n",
    "#     }\n",
    "    \n",
    "#     with YoutubeDL(ydl_opts) as ydl:\n",
    "#         ydl.download([video_url])\n",
    "\n",
    "#     # Create a Pandas DataFrame from the data\n",
    "#     df = pd.DataFrame([data])\n",
    "\n",
    "#     # Save the DataFrame to a CSV file in the 'metadata' folder with the title as the file name\n",
    "#     csv_file_path = os.path.join(metadata_folder, f\"{sanitized_title}.csv\")\n",
    "#     df.to_csv(csv_file_path, index=False)\n",
    "#     print(f\"Metadata saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f88bb-39ef-47d8-b442-3b09e374e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapetube\n",
    "import yt_dlp\n",
    "\n",
    "\n",
    "\n",
    "channel_id = \"UCzgxx_DM2Dcb9Y1spb9mUJA\"\n",
    "videos = scrapetube.get_channel(channel_id)\n",
    "channel_name = 'Twice'\n",
    "# Initialize an empty list to store video data\n",
    "video_data = []\n",
    "# Create a yt-dlp object\n",
    "ydl = yt_dlp.YoutubeDL()\n",
    "# Initialize a counter to limit the number of videos\n",
    "count = 0\n",
    "for video in videos:\n",
    "    if count < 2:  # Limit to the first two videos\n",
    "\n",
    "        video_id = video['videoId']\n",
    "        date = video['publishedTimeText']\n",
    "        video_links = f\"https://www.youtube.com/watch?v={id}\"\n",
    "        # Extract video information\n",
    "        info_dict = ydl.extract_info(video_links, download=False)\n",
    "    \n",
    "        # Extract the desired information\n",
    "        title = info_dict.get('title', '')\n",
    "        description = info_dict.get('description', '')\n",
    "        views = info_dict.get('view_count', 0)\n",
    "        likes = info_dict.get('like_count', 0)\n",
    "\n",
    "\n",
    "        \n",
    "        import json\n",
    "        \n",
    "        # Define the filename where your JSON data is stored\n",
    "        json_filename = 'video_id.json'\n",
    "        \n",
    "        # Step 1: Open the JSON file and load its contents into a Python dictionary\n",
    "        with open(json_filename, 'r') as json_file:\n",
    "            j_data = json.load(json_file)\n",
    "        \n",
    "        # Step 2: Update the 'video_id' key with a list of video IDs\n",
    "        if 'video_id' not in j_data:\n",
    "            j_data['video_id'] = []  # Initialize an empty list if 'video_id' doesn't exist\n",
    "        \n",
    "        # Add video IDs to the 'video_id' list if they don't already exist\n",
    "        video_ids_to_add = [video_id]\n",
    "        \n",
    "        for video_id in video_ids_to_add:\n",
    "            if video_id not in j_data['video_id']:\n",
    "                j_data['video_id'].append(video_id)\n",
    "        \n",
    "        # Step 3: Save the updated dictionary back to the JSON file\n",
    "        with open(json_filename, 'w') as json_file:\n",
    "            json.dump(j_data, json_file)\n",
    "        \n",
    "        print(f\"Updated JSON data saved to {json_filename}\")\n",
    "\n",
    "\n",
    "        all = {\n",
    "            'channle_name': channel_name,\n",
    "            'channle_id':channel_id,\n",
    "            'title':title,\n",
    "            'description':description,\n",
    "            'view':views,\n",
    "            'like':likes,\n",
    "            'published_date':date,\n",
    "            'link':video_links,\n",
    "            'video_id':video_id\n",
    "            \n",
    "        }\n",
    "        video_data.append(all)\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Now you have video information stored in the video_data list\n",
    "for video in video_data:\n",
    "    print(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdebd0e2-675f-41e6-8012-7c687b4bf77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420a7ac-6466-40a0-87a4-f2b134f77fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e044c-60ce-4060-8588-9ad363f202dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c9c9b-b73f-438f-83e1-f7a2a91d04b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a3df4c-fb53-4bd5-b18d-648c81f003c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
